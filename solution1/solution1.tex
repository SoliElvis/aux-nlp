
\input{premath.tex}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{titling}
\usepackage{amsmath}
\usepackage{fancyhdr}
\pagestyle{fancy}

\title{Solutionnaire 1}
\author{Frederic Boileau}
\date{\today}



\begin{document}
\fancyhf{}
\fancyhead[L]{Dynamic programing}
\maketitle
\thispagestyle{fancy}

J'introduis quelques notations. En analyse convexe on dénote l'ensemble
des fonctions convexes \emph{propres} sur un espace euclidien $\mathbb E$
(dans notre cas simplement $\mathbb R^n$) par $\Gamma (\mathbb E)$.
\begin{equation*}
    \Gamma (\mathbb E) \triangleq \{f : \mathbb R^n \rightarrow \mathbb R
    \cup \{+\infty\}, \text{f propre et convexe}\}
\end{equation*}
Une fonction \emph{propre} est une fonction pour laquelle il existe au moins un
$x$ t.q. $f(x) < + \infty $ et pour laquelle pour tout $x$ $f(x) > -\infty$. 
On écarte ainsi certains cas pathologiques.

J'utilise la notation de Evans pour la d\'erivation. Ainsi $D_x f$ 
est la d\'eriv\'ee de f par rapport \`a x, si c'est un nombre, un vecteur
(comme pour le gradient) ou une matrice (jacobien) d\'epend du contexte
(de $f$ et de $x$) dans tous les cas $D$ est toujours une approximation
linéaire de f. 

Soit A une matrice. Si A est d\'efinie positive  (resp.\  semi-d\'efinies
positives) on \'ecrit $A \succ 0$ (resp $A \succeq 0$).

\section{Direction de Descente}

Soit $f$ une fonction convexe différentiable sur $\mathbb R^n$ 
et $x$ et $y$ deux points de $\mathbb R^n$ t.q. $f(y) < f(x)$.
Montrer que $y-x$ est une direction de descente de $f$ en $x$.
Notons qu'aucun ordre n'est impos\'e  sur $x$ et $y$.\\

La preuve d\'ecoule directement de la d\'efinition de la 
convexit\'e donn\'ee en cours:
\begin{align*}
    f(\alpha y + (1-\alpha)x) &\leq \alpha f(y) + (1-\alpha)f(x)\\
    f(x + \alpha(y-x)) &\leq \alpha f(y) + (1-\alpha)f(x)\\
    f(x + \alpha(y-x)) &< \alpha f(x) + (1-\alpha)f(x)\\
    f(x + \alpha(y-x)) &< f(x)
\end{align*}

\clearpage
\section{Syst\`eme lin\'eaire et optimisation}
Prouvez que tout système linéaire $Ax = b$, avec A définie positive, peut se
reformuler comme un problème d’optimisation sans contrainte. 

Pour la reformulation nous avons plusieurs crit\`eres; premi\`erement on veut
que le minimum global corresponde \`a la solution, dans un second temps
si le probl\`eme est convexe nous avons des garanties quant \`a la recherche
de ce minimiseur global. Il y a deux formulations simples, je traite les
deux.
\subsection{Forme Quadratique}
Le reformulation la plus simple est de prendre la forme quadratique suivante:\\
\begin{equation}
    f(x) = \frac{1}{2}x\tran A x - x\tran b
\end{equation}

Le calcul de d\'eriv\'es par rapport \`a des vecteurs ou des matrices
peut \^etre laborieux mais l'identit\'e suivante est tr\`es importante:
\begin{equation}
    D_x (x\tran A x) = x\tran (A + A\tran)
\end{equation}
Ainsi nous avons:
\begin{equation}
    D_x(f) = Ax - b \qquad D_x^2(f) = A \succ 0
\end{equation}
La premi\`ere \'equation montre que le gradient de la fonction est nul ssi le
$x$ est une solution du syst\`eme lin\'eaire. La deuxi\`eme \'equation montre
que la deuxi\`eme d\'eriv\'ee (le hessien) de $f$ par rapport est
\emph{d\'efinie positive} (par supposition sur A). La fonction est donc
strictement convexe et atteint son minimum ssi le gradient est nulle, ce qui
n'arrive que lorsque $Ax = b$. \\

Notons que pour une fonction \`a une variable $D^2_x(f) > 0$ implique
que $f$ est strictement convexe en $x$. Pour une fonction \`a plusieures
variables, $f : \mathbb R^n \rightarrow \mathbb R$, nous avons que $D^2_x(f) =
\nabla^2_x (f) \succ 0$ implique $f$ strictement convexe. Nous voyons donc
que la notation est appropri\'ee puisque l'on g\'en\'eralise la notion de
`plus grand que z\'ero.'

\clearpage

\subsection{Norme euclidienne}
Une autre fonction objectif qui a été choisi par certains étudiants 
est la suivante:
\begin{equation}
    f(x) = \frac{1}{2} \norm{Ax - b}^2_2 \label{norm}
\end{equation}
Cette fonction est aussi convexe et d'ailleurs $C^2$. De plus la convexit\'e ne
d\'epend \emph{pas} du fait que la matrice $A$ en tant que telle soit pos-def
(definie positive). La preuve est cependant moins directe.\\

Soit $h(x) = \frac{1}{2}\norm x^2_2$ et $g(x) = Ax -b$
et donc $f(x) = h(g(x))$.\\

D'abord nous calculons le gradient de la norme euclidienne au carr\'e.
\begin{equation}
    D_{x_i}h(g(x)) =  D_{x_i} \frac{1}{2}\norm {g(x)}_2^2
    = D_{x_i}\, \frac{1}{2} \langle g(x),g(x)\rangle
    = D_{x_i}\, \frac{1}{2} \sum g{(x_i)}^2
    =  g'(x_i)g(x_i)
\end{equation}
Nous avons donc $ D \frac{1}{2} \norm x_2^2 = x\tran$ ($g$ est la fonction
qui ne fait rien, la fonction identit\'e.) La transpose vient du
fait que $Dh$ est une application lin\'eaire sur $\mathbb R^n$ qui approxime la
fonction $h$. Ainsi une notation plus claire serait $D h(x) = \langle x,
\cdot\rangle = x\tran$\\


Maintenant nous pouvons appliquer la d\'erivation en chaine pour
trouver la d\'eriv\'ee de la fonction\eqref{norm}.
\begin{equation}
    f'(x) = h'(g(x))g'(x) = g'(x)\tran g(x) = A\tran (Ax-b)
\end{equation}

Finalement nous avons donc 
\begin{equation}
    D^2 f = A\tran A
\end{equation}
Or c'est un fait connu en alg\`ebre lin\'eaire que 
pour toute matrice $X$ invertible $X\tran X \succ 0$. Nous pouvons le
d\'emontrer rapidement:
\begin{equation}
    x\tran AA\tran x = (A\tran x)\tran (A\tran x) = \norm {Ax}_2^2 
\end{equation}
Si $A$ est invertible alors clairement $\norm {Ax}^2_2 > 0$ pour tout
$x\neq 0$. Au final nous avons donc que $D^2 f(x) \succ 0$ est la fonction
objectif est donc strictement convexe avec comme minimum global la solution
du syst\`eme lin\'eaire.\\

\textbf{Notes}:\\
Cette petite preuve montre l'approche la plus directe pour trouver le gradient
d'une fonction, c'est \`a dire trouver les d\'eriv\'ees partielles pour un
indice arbitraire et utiliser les regles de d\'erivation en chaine.\\
De plus; $x\tran$ est fondamentalement un autre objet que $x$\footnote{pour
ceux que ca interesse le premier est un element de l'espace des applications
lineaires sur $\mathbb R^n$, espace que l'on appelle le duel}

$x\tran$ est une \emph{fonction} qui prend un vecteur et retourne un nombre,
$x$ est un vecteur.




\end{document}

